---
title: "stock_market"
author: "Joshua Eason"
date: "3/14/2020"
output: html_document
---

```{r setup, include=FALSE}

#Dependencies... If you don't have these libraries, install them using the code below...
#install.packages("tidyverse")
#install.packages("caret")
#install.packages("mosaic")
#install.packages("skimr")
#install.packages("corrplot")
#install.packages("zoo")
#install.packages("RCurl")
#install.packages("rvest")
#install.packages("robotstxt")

library(tidyverse)
library(caret)
library(mosaic)
library(skimr)
library(corrplot)
library(zoo)
library(RCurl)
library(rvest)
library(robotstxt)

data <- read.csv("GSPC.csv")

```

This is my quick analysis of your dataset, with some ideas and sample implementations. All of this is obviously in R since it is clearly the best language and R users are the superior race of scientists. Ill get started. 

First, I want to know what my data "looks" like. Meaning, what are the types of data, etc. For that I will just do a glimpse of my data. skim lets me get some summary statistics. I am using the "skimr" library. This is mainly to just get an idea of what I am dealing with.

As you can see, here the output gives me the mean, min, max, quartiles, and a really bad histogram. Luckily for us, there isnt a lot of cleaning that needs to happen because all of the numeric variables are already formatted properly. I do want to make one change to the date variable. 

```{r}

skim(data)

```

In this dataset, date is dealt with as a factor, meaning it is categorical in nature. This will probably be a problem when we try to go after the time-series analysis. Ill fix that here...

```{r}

data$Date <- as.Date(data$Date)

skim(data$Date)

```


So, we obviously suspected some exponential growth. I have transformed the closing price on a log scale and plotted its relationship with time here, and you were right. It is, and now we have it linearized for what it is worth. 

```{r}

data_log <- data %>% mutate(log_close = log(Close))

data_log %>% ggplot(aes(x = Date, y = log_close)) + geom_point()

model_lm_1 <- lm(Close ~ Volume,data = data)
summary(model_lm_1)

data %>% ggplot(aes(x = Date, y = log(Volume))) + geom_point() + geom_smooth(mmethod = "lm")
data %>% ggplot(aes(x = log(Volume), y = log(Close))) + geom_point() + geom_smooth(mmethod = "lm")

```


Here, I am going to build a rolling window for you. Ill do one for each: 7, 14, 30, 180, 365 days. That should be enough...

```{r}

data_roll <- data %>% mutate(roll_7_close = rollapply(data$Close, width = 7, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_7_vol = rollapply(data$Volume, width = 7, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_14_close = rollapply(data$Close, width = 14, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_14_vol = rollapply(data$Volume, width = 14, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_30_close = rollapply(data$Close, width = 30, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_30_vol = rollapply(data$Volume, width = 30, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_180_close = rollapply(data$Close, width = 180, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_180_vol = rollapply(data$Volume, width = 180, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_365_close = rollapply(data$Close, width = 365, by = 1, FUN = mean, align = "right", partial = TRUE),
                             roll_365_vol = rollapply(data$Volume, width = 365, by = 1, FUN = mean, align = "right", partial = TRUE))

```


For an interesting aside, I'd like to explore the idea of automating your data downloads in order to give you the ability to run experiments on your model. I'll take a shot at it here, and I am leaving this code in no matter what cause I think you could benefit from seeing how I did it either way. 

```{r}
url <- "https://www.advfn.com/nyse/newyorkstockexchange.asp"

page <- read_html(url)
 
page_table_data <-page %>%
   html_nodes("a") %>%
   html_text()

company = as.character(page_table_data[(seq(from=36, to=252, by=3))])
```


update... This code works. It dumps the downloaded file into the working directory you have set in R. There are a couple things that need to generalize in order to run this code on "any" company. 

1. Individual companies: "https://query1.finance.yahoo.com/v7/finance/download/*insertCallNameHere*?period1=*epochTimeStart*&period2=*ephochTimeEnd*&interval=1d&events=history&crumb=yNsTEUCS33"

2. Indices: "https://query1.finance.yahoo.com/v7/finance/download/%5E*insertCallNameHere*?period1=*epochTimeStart*&period2=*ephochTimeEnd*&interval=1d&events=history&crumb=yNsTEUCS33"

for the "epoch time" start and end, this will be in number of seconds since the epoch. You are essentially telling the website how many seconds of data you are interested in. 

Once you have your file downloaded, you can do the same stuff to it that I did to the file you sent me above if you want. 

```{r}
url_1 <- "https://query1.finance.yahoo.com/v7/finance/download/" 
url_2 <- "?period1="
url_3 <- "&period2="
url_4 <- "&interval=1d&events=history&crumb=yNsTEUCS33"
call <- "UAL" # youd want to make this something that can change based on the vaule of the current company in the vector

# The code below gives you a 10 year window. This code will crash if you try and get more time than the company has been around. 
start <- "1268641665" # if you want to vary this based on how long the company has been on the market, this would need to change
end <- as.character(as.integer(as.POSIXct(Sys.time()))) # this needs to stay the same based on "today's date"

# This builds the download link. 
URL <- paste(url_1, call, url_2, start, url_3, end, url_4, sep = "")

# This downloads the file and writes out a csv file for you to your working directory. 
call_file <- paste(call, ".csv", sep = "")
download.file(URL,destfile=call_file,method="libcurl")

# I read in the test file for you here so you could see it. 
call_data <- read.csv(call_file)
head(call_data)

```

```{r}
# This is the code  that finds the list of company names. This is literally one of the first links that I went to. Id call this more of a proof of concept to show that it will work. If you want to make it more robust, you could find a better place to scrape from. I can help you look, but I dont want to sink a bunch of time into it if you arent planning on using it.
url <- "https://www.advfn.com/nyse/newyorkstockexchange.asp"

page <- read_html(url)
 
page_table_data <-page %>%
   html_nodes("a") %>%
   html_text()

company = as.character(page_table_data[(seq(from=36, to=252, by=3))])

remove <- c("RDS.A","BRK.B")
company <- company[!(company %in% remove)]

# This loop does the experiment on each company.
for(call in company) {
  
  call_name <- call
  
  url_1 <- "https://query1.finance.yahoo.com/v7/finance/download/" 
  url_2 <- "?period1="
  url_3 <- "&period2="
  url_4 <- "&interval=1d&events=history&crumb=yNsTEUCS33"
  start <- "1552788304" # This only goes back like one year. If you want more, you need to get the rest by adjusting this For each year further you want to subtract another 31536000 seconds for each additional year that you want to look back in time. I only did one year cause you are pretty much guaranteed that all the companies that I scraped would have at least a year of stock data. If you wanted to get really fancy and you could find a place that I could scrape from where it would tell me how long the company had been on the exchange, you would be able to set up a custom range for each company, although I wouldnt recommend that if you are looking to experiment on this data. Here, there are 71 compnaies, and we have like 1 year of data for each.
  end <- as.character(as.integer(as.POSIXct(Sys.time())))

  URL <- paste(url_1, call_name, url_2, start, url_3, end, url_4, sep = "")

  call_file <- paste(call_name, ".csv", sep = "")
  download.file(URL,destfile=call_file,method="libcurl")
  
  #do some fancy data science shit here... 
  #store the results of your fancy data science shit here... 
  #store the accuracy of the fancy results here. This is actually really easy from a code perspective and if you decide to go this route, I can have you set up in a matter of minutes. 
  
  unlink(call_file) # This deletes the original data file once you are done. No need to store it...
  
  }

```
UPDATE: I basically built this as a prototype above. 

So you now have the code to download your files automatically, provided you do some more legwork. Lastly, I'll show you how I'd do the experiment. This is gonna be hardcore pseudocode...

This code wont run... like... at all... 

Option 1: build a model for each comany, then test on that model
```{r}

for (comany in company_vector) {
  
  1. download your dataset and write a csv file
  2. read in that file and create a matrix
  3. do your cleaning and transformations on the data
  4. build your model on the data (train)
  5. make your predictions and save to a new dataset for just this company with probably only prediction and actual (test)
  5. measure your prediction accuracy, and create a new dataset that you will start appending these metrics to for each company
  
}

```

Option 2: Build a model from the composite data from a representative sample of companies and train and test on that model, then use that one single model to make predictions on a series of "new" companies that you have ground truth data to verify accuracy. 


Here, I am gonna start my analysis. It is time-series data, and I want to preserve the ordinality of it all, but I think looking at the data more locally will give us a better idea about what we are dealing with. Im going to start with a basic regression model. 

```{r}

data_roll <- data %>% mutate(roll_30_close = rollapply(data$Close, width = 30, by = 1, FUN = mean, align = "right", partial = TRUE),roll_30_vol = rollapply(data$Volume, width = 30, by = 1, FUN = mean, align = "right", partial = TRUE))

train_index <- createDataPartition(data_roll$Close,
                                   times = 1,
                                   p = .75,
                                   list = FALSE)

train_data <- data_roll[train_index,]
test_data <- data_roll[-train_index,]

processed <- preProcess(data_roll, method = c("center", "scale"))
normalized_data <- predict(processed, data_roll)

train_normalized <- normalized_data[train_index,]
test_normalized <- normalized_data[-train_index,]

model_glm <- train(form = Close~ roll_14_vol,
               data = train_normalized,
               method = "glm")

test_pred_glm <- test_normalized %>% mutate(close_pred = predict(model, test_normalized),
                                  pred_diff = close_pred - Close)

glm_rmse_train <- max(model_glm$results$RMSE)
glm_r2_train <- max(model_glm$results$Rsquared)
glm_mae_train <- max(model_glm$results$MAE)
test_normalized <- test_normalized %>% mutate(close_pred_glm = predict(model_glm, test_normalized))
glm_rmse_test <- RMSE(test_normalized$close_pred_glm, test_normalized$Close)
glm_r2_test <- R2(test_normalized$close_pred_glm, test_normalized$Close)
glm_mae_test <- MAE(test_normalized$close_pred_glm, test_normalized$Close)

glm_stats <- tibble(glm_rmse_train = glm_rmse_train, glm_rmse_test = glm_rmse_test, glmr2_train = glm_r2_train, glmr2_test = glm_r2_test,
                    glm_mae_train = glm_mae_train, glm_mae_test = glm_mae_test)

knitr::kable(glm_stats)

# alpha <- seq(from=0, to=0, length=11)
# lambda <- seq(from=0, to=1, length=11)
# grid <- expand.grid(alpha, lambda)
# colnames(grid) <- c("alpha", "lambda")
# 
# 
# model_net <- train(form = Close~ roll_7_vol,
#                data = train_data,
#                method = "glmnet",
#                tuneGrid = grid)
# 
# test_pred_net <- test_data %>% mutate(close_pred = predict(model, test_data),
#                                   pred_diff = close_pred - Close)

```

